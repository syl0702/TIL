# Machine Learning 수학 요약
## 스칼라 벡터
1. 집합은 순서가 없다 = 순번이 없다 = 인덱스 없다.
    - 집합에서의 아래 첨자는 인덱스가 아니라 아이디. 고유값.
        - x들을 구별해야 해서.
2. 실수 집합 R
    - 이 세상에 존재하는 모든 실수들에 대한 집합
    - (x1, x2) 숫자 쌍(세트) E R*R로 표현.
    - 보통 제곱으로 사용한다.

3. 데이터: 수집된 값 (이를 모으면 정보가 된다.)
    - 스칼라: 타겟 1개, 종류 1개
    - 벡터
        - 타겟:N개 종류: 1개
        - 타겟: 1개 종류: N개

        - 특징 벡터(Feature Vector)
            - 데이터 벡터가 예측 문제에서 입력 Data로 사용하는 것.
            - X-> Y 할 때 오차를 작게 하는 "학습"을 시킨다.

4. 행렬: 타겟 N개, 종류 M개
    - 행렬을 배열로 표현해.

5. Tensor: 3차원
    - 스칼라: 0-Rank T
    - 벡터: 1-Rank T
    - 행렬: 2-Rank T
6. 전치 행렬
7. 대각 행렬
8. 항등 행렬- 대각 선분이 1
9. 대칭 행렬은 정방 행렬만 가능하다.

10. 덧셈과 곱셈을 하는 이유는 Data의 요약을 위한 것.
    - 덧셈은 주로 서로 연관성이 없는 데이터를 요약할 때, 곱셈은 주로 연관성이 있는 데이터를 요약할 때 사용한다.

## 선형 회귀
1. 선형 회귀 식에서 w는 데이터를 강조하는 부분으로 가중치라고도 한다.
2. 이러한 가중치가 높을 경우 x가 조금만 높아져도 급격히 올라간다.
3. 즉, x 값을 어떻게 반영할 것인가를 보여주는 것.

## 선형 조합
- 벡터/행렬에 스칼라값을 곱한 후 더하거나 뺀 것을 벡터/행렬의 선형 조합(linear Combination)이라고 함.


## 잔차
1. 잔차는 error 즉 오차를 줄이기 위한 것.

## 잔차 제곱합
- 잔차 제곱합을 구하는 이유는 **음수를 없애기 위해서**이다.
- 오차 관리는 하나로 해야 하기 때문에 오차들을 다 더해야 한다.
- 잔차가 음수 항목으로 인하여 합이 0이 되는 것을 막기 위해 제곱이 아닌 절댓값을 사용할 수도 있으나, 오차를 쉽게 고치기가 어렵다.
- 그러나 제곱합을 할 경우, 오차가 났을 때 그 숫자가 엄청나게 올라가기 때문에 오차를 수월하게 고칠 수 있다.

## 정부호와 준정부호
- 일반적으로 대칭 행렬에 대해서만 정의.
- 대표적인 예시는 항등행렬 I

## 행렬 Norm 중요
- Norm: 벡터의 길이 혹은 벡터의 크기를 나타냄.
- p = 2인 경우가 가장 많이 쓰이는 식 (이때 프로베니우스 놈)이라 부르며 다른 표기를 하기도 함.
- Norm은 항상 0보다 같거나 크다.
- 벡터의 norm의 제곱이 벡터의 제곱합과 같다.

## 역행렬
- 역행렬이 존재하는 행렬: 가역행렬, 정칙행렬, 비특이행렬로 불림.
- 역행렬이 존재하지 않는 행렬: 비가역행렬, 특이행렬, 퇴화 행렬로도 불림.
- **역행렬은 행렬식이 0이 아닌 경우에만 존재한다**

## 선형 예측 모형
- Xw = y
- 이 예측 모형의 가중치 벡터 w를 찾는 것은 계수행렬이 X, 미지수 벡터가 w, 상수벡터가 y인 선형 연립 방정식의 답 찾는 것.

## 미지수의 수와 방정식의 수
1. 방정식의 수가 미지수의 수와 같다. (N = M)
2. 방정식의 수가 미지수의 수보다 적다. (N < M)
    - 무수히 많은 해가 존재할 수 있다.
3. 방정식의 수가 미지수의 수보다 많다. (N > M)
    - 모든 조건을 만족하는 해가 하나도 존재할 수 없을 가능성이 있음.
    - 선형 예측모형을 구할 때 이와 같은 경우가 많다.
    - 따라서 선형 연립 방정식을 푸는 방식으로는 선형 예측모형의 가중치 벡터를 구할 수 없다.

## 최소 자승 문제
이러한 상황에서의 해결책: **정확하게 똑같지 않아도 된다면?**
- 비슷하게 예측하기!
- 좌변과 우변의 차이를 최소화하는 문제로 바꾸어 풀기.
- 예측값과 목푯값의 차이는 잔차(residual)
- 잔차는 벡터이기에 최소자승문제에서는 벡터의 크기 중에 벡터 norm을 최소화 하는 것.
- 식에서 argmin은 함수 f(x)를 가장 작게 만드는 x값을 의미.

## 벡터의 길이
- Norm의 제곱은 벡터의 제곱합


## 단위 벡터 중요!!
- (Norm) 길이가 1인 벡터를 **단위 벡터**라고 한다.

## Word2Vec (텍스트 인코더)
- 단어에 대한 벡터를 만들어 준다.

## 유클리드 거리
- 증명 다시 한번 해볼 것.

## 내적과 삼각함수
- 코사인 유사도를 통해 유사한 것을 찾을 수 있게 된다.
- 코사인 1이면 유사도 높고, 0은 유사도 낮다